---
title: "Assignment 02"
author: "Dillon Otto"
date: "3/14/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(ggplot2)
```

## Introduction

The University of Utah College of Engineering keeps 75 linux machines for student and faculty use.  Since there are so many machines, and IT resources are fixed, it is useful to have automated processes to monitor critical statistics about these machines and to report anomalous behavior to the IT department.  In order to facilitate our ability to identify machines that might be at risk, it is crucial to know a *baseline* state of the machines to which we can make useful comparisons.  In this study, we will investigate ping times, page fault rates, core temperatures and packet receipt rates when machines are expected to be "at rest" in order to establish such a baseline.  We will further investigate a baseline for integrity of the system's `openssl` AES encryption to provide a baseline for evidence that encryption libraries might be compromised.  We hope that your investigations now will provide a solid foundation for the future creation of automated processes that give insight into the health and stability of lab machines in the University of Utah College of Engineering.

## Data Sources

The data for our baseline was collected from CADE lab machine 1-13 over five consecutive hours from 8pm-1am. We want to apply this baseline to all 75 CADE lab machines. This is not a simple random sample, or even close.  All the data was gathered from a single machine, and in one limited time range. However, we ensured that no one else was using machine 1-13 during our study time by using the top command to see if any intensive processes were being run. Therefore, assuming that machine 1-13 was operating normally, this data may still serve as a decent baseline despite being a poor sample.   

## Data Collection

As previously mentioned, data was gathered about five key statistics:

**1. Ping Times: **
An individual ping time represents the time it takes to 'ping' (send a packet and receive a response) to CADE machine 1-24. This is done using the UNIX command *ping* and automated using a python script to send 5 pings per second for an hour.  

```{r pingData}
ping_data = read.csv("ping_data.csv")
knitr::kable(head(ping_data), row.names=FALSE, align="l")
```

**2. Page Fault Rates: **
An individual reading shows a total count of page faults. This is done by reading the stat in /proc/vmstat and automated using a python script to read the count 5 times per second for an hour.
```{r pageData}
page_data = read.csv("page_fault_data.csv")
knitr::kable(head(page_data), row.names=FALSE, align="l")
```

**3. Core Temperatures: **
An individual temperature reading contains the core temperature, in degrees Celsius, of each of the 8 CPU cores. This is done using the Linux command *sensors* and automated using a python script to read the temperatures 5 times per second for an hour.   

```{r coreData}
temp_data = read.csv("temp_data.csv")
knitr::kable(head(temp_data), row.names=FALSE, align="l")
```

**4. Packet Data: ** An individual reading shows a total count of packets received This is done with the unix *netstat* command and automated using a python script to read the count 5 times per second for an hour.

```{r packetData}
packet_data = read.csv("packet_data.csv")
knitr::kable(head(packet_data), row.names=FALSE, align="l")
```

**5. OpenSSL Encryption Test: ** Four data sets were collected using python scripts: (1) the bit values in the total collection of Shakespeare's works; (2) the bit values after encyption; (3) the byte values of the works; (4) the byte values after encryption.

## Data Analysis

Unfortunately, the data generated by the *nix commands are not all in a very readable format. Particularly, the number of packets data and page fault data were recorded as overall counts every fifth of a second. This data would be easier to work with as a count of packets per second and page faults per second. We can accomplish this by grouping the data values to their respective time and subtracting the highest and lowest (or first and last) total. This gives much more workable datasets:
```{r}
packetDataClean = read.csv("packet_data.csv",header = TRUE) %>%
  mutate(time = as.integer(time)) %>%
  group_by(time) %>%
  summarise(packets = max(total_packets) - min(total_packets))

pagefaultDataClean = read.csv("page_fault_data.csv",header = TRUE) %>%
  mutate(time = as.integer(time)) %>%
  group_by(time) %>%
  summarise(faults = max(total_pgfaults) - min(total_pgfaults))

knitr::kable(head(packetDataClean), row.names=FALSE, align="l")
```

**Received Packets**
```{r}
packetMean = mean(packetDataClean$packets)

packetSd = sd(packetDataClean$packets)

```

Summary Statistics:

Mean: `r packetMean`

Standard Deviation: `r packetSd`

```{r}
packetFrame <- data.frame(xx = packetDataClean$packets)

ggplot(packetFrame,aes(x=xx)) + 
    geom_histogram(data=packetFrame,fill = "red", alpha = 0.2, breaks = seq(0, 100, by = 1)) + 
    labs(x="Number of Packets/Second", y="Frequency")
```

Packets per second is discrete data. You can't send half a packet. The data is clearly not a uniform or binomial distribution. However, it seems reasonable that it could be represented by a poisson distribution. When a computer isn't in use, are the number of packets affected by the number that came the second before? It seems plausible that they aren't.

```{r}
plot(0:30, dpois(0:30, mean(packetDataClean$packets)), xlab="Packets/second", ylab="Frequency", main="Packets vs Poisson")
hist(packetDataClean$packets, freq=FALSE, add=TRUE, col=rgb(0,0,0,alpha=0.2), breaks = seq(0,1000,by=1))
```

We can see our experimental data is skewed well left of the Poisson distribution. However, there is a wide deviation from the mean of `r packetMean`. While most of the numbers are clustered from 0-10 (~96%), there is a non-negligible number of values from 11-30 (~3%) and 31-100 (~0.7%), and a few significant outliers over 100 (~0.3%). 

Considering the number of packets that significantly differ from the mean, it seems questionable to remove them. However, even if we remove all values above 30, we get the graph below:

```{r}
withoutOver30 <- packetDataClean$packets[packetDataClean$packets < 30]
plot(0:30, dpois(0:30, mean(withoutOver30)), xlab="Packets/second", ylab="Frequency", main="Packets vs Poisson (without values > 30)")
hist(withoutOver30, freq=FALSE, add=TRUE, col=rgb(0,0,0,alpha=0.2), breaks = seq(0,1000,by=1))
```

The data is *still* skewed significantly left of the poisson. Packets/second is not represented by any clean statistical distribution. My guess is that the computer is performing intermittent operations (like checking for updates), which leads to the higher packet values and prevents the data from being Poisson (as any given second is not the same).

**Ping Times**
```{r}
pingDataClean = read.csv("ping_data.csv",header = TRUE) %>%
  mutate(ping = as.numeric(str_split(ping,pattern = " ",simplify = TRUE)[,1]))

pingMean = mean(pingDataClean$ping)

pingSd = sd(pingDataClean$ping)

```

Summary Statistics:

Mean: `r pingMean`

Standard Deviation: `r pingSd`

```{r}
pingFrame <- data.frame(xx = pingDataClean$ping)

ggplot(pingFrame,aes(x=xx)) + 
    geom_histogram(data=pingFrame,fill = "red", alpha = 0.2, breaks = seq(0, 0.7, by = 0.01)) + 
    labs(x="Ping (ms)", y="Frequency")
```

Ping is continuous data. From the histogram, the data looks like it could be a normal distribution. While this may be a good fit, this can't be the distribution as it would allow the possibility of negative values. However, a gamma distribution follows a similar curve and does not allow negatives. 

```{r}
scale <- var(pingDataClean$ping)/pingMean
shape <- pingMean*pingMean/var(pingDataClean$ping)
plot(seq(0, 1, by = 0.01), dgamma(seq(0, 1, by = 0.01), shape=shape, scale=scale), xlab="Ping (ms)", ylab="Frequency", main = "Ping vs Gamma")
hist(pingDataClean$ping[pingDataClean$ping < 1], freq=FALSE, add=TRUE, col=rgb(0,0,0,alpha=0.2), breaks = seq(0, 1, by = 0.01))

```

Sure enough, the gamma distribution appears to provide a good fit for the ping data. 

**Page Faults**
```{r}
pageMean = mean(pagefaultDataClean$faults)

pageSd = sd(pagefaultDataClean$faults)

```

Summary Statistics:

Mean: `r pageMean`

Standard Deviation: `r pageSd`

```{r}
pageFrame <- data.frame(xx = pagefaultDataClean$faults)

ggplot(pageFrame,aes(x=xx)) + 
    geom_histogram(data=pageFrame,fill = "red", alpha = 0.2, breaks = seq(1000, 4000, by=5)) + 
    labs(x="Page Faults per second", y="Frequency")
```

Page faults are discrete data. At first glance, it looks like the data could be Poisson. It would seem reasonable that the probability of page faults could be the same at any given second. However, if we look closer at the graph, we can see that in addition to most of the values falling at ~1400, there are several additional smaller clusters. This explains the very high standard deviation -- and suggests that the page faults may be dependent on something (like what task the computer is executing at that time). Therefore, like packets, it seems likely that page faults will not be modeled by a statistical distribution. 

```{r}
plot(1000:2000, dpois(1000:2000, mean(pagefaultDataClean$faults[pagefaultDataClean$faults < 60000])), xlab="Page Faults/second", ylab="Frequency", main="Page Faults vs Poisson")
hist(pagefaultDataClean$faults[pagefaultDataClean$faults < 60000], freq=FALSE, add=TRUE, col=rgb(0,0,0,alpha=0.2), breaks = seq(0,60000,by=5))
```

Sure enough, the data is skewed left from a Poisson distribution. 


**Core Temperatures**
```{r coreSummaryStats, echo=FALSE}
means = c(mean(temp_data$X0), mean(temp_data$X1), mean(temp_data$X2), mean(temp_data$X3), mean(temp_data$X4), mean(temp_data$X5), mean(temp_data$X6), mean(temp_data$X7))

sds = c(sd(temp_data$X0), sd(temp_data$X1), sd(temp_data$X2), sd(temp_data$X3), sd(temp_data$X4), sd(temp_data$X5), sd(temp_data$X6), sd(temp_data$X7))

meanFrame = data.frame(
  Core = c("X0", "X1", "X2", "X3", "X4", "X5", "X6", "X7"),
  Mean = means
)

sdFrame = data.frame(
  Core = c("X0", "X1", "X2", "X3", "X4", "X5", "X6", "X7"),
  StdDev = sds
)
```

Summary Statistics:
```{r}
knitr::kable(meanFrame, row.names=FALSE, align="l")
```

```{r}
knitr::kable(sdFrame, row.names=FALSE, align="l")
```

Based on the above means and standard deviations for each core, there is no obvious disparity between cores (e.g. the majority of work being assigned to X0). As such, for the sake of simplicity, we will combine all of the cores temperature data into a single data set. The results of that set are summarized below. 

Combined mean: `r mean(means)`

```{r}
coreTempData = c(temp_data$X0, temp_data$X1,temp_data$X2,temp_data$X3,temp_data$X4,temp_data$X5,temp_data$X6,temp_data$X7)
coreTempFrame <- data.frame(xx = coreTempData,yy = rep(letters[1:8],each = 17808))


ggplot(coreTempFrame,aes(x=xx)) + 
    geom_histogram(data=coreTempFrame,fill = "red", alpha = 0.2, bins = 30) + 
    labs(x="Temperature", y="Frequency")
```

Temperature is a continuous variable. Based on what we know about temperature, it would not make sense for the data to be an exponential distribution. Temperature is not "memoryless." If the temperature of a core was 60 degrees in the previous second, it will be relatively close in the current second. It takes time to build and dissipate heat. We expect the CPU cores to stay close to a constant temperature, as they are doing a relatively constant amount of work, with occasional fluctuations to higher and lower values. As we would expect the higher and lower values to be less frequent, a uniform distribution would not fit the data (and obviously does not looking at the graph). However, that type of pattern may be modeled by the normal distribution. 

```{r}
plot(30:50, dnorm(30:50, mean(means)), xlab="Temperature", ylab="Frequency", main = "Temperature vs Normal")
hist(coreTempData, freq=FALSE, add=TRUE, col=rgb(0,0,0,alpha=0.2))
```

Sure enough, the normal distribution is a good fit for the experimental data, as shown in the graph above. 

**Shakespeare Bits and Bytes**

Summary Statistics:

Shakespeare Bits: 26237329 zero, 19816255 one

Shakespeare Encrypted Bits: 23025323 zero, 23028437 one


```{r}
sBitsFrame <- data.frame(xx = c(0,1,0, 1), yy = c(26237329, 19816255, 23025323, 23028437), z = c("Plain", "Plain", "Encrypted", "Encrypted"))
ggplot(sBitsFrame,aes(x=xx, y=yy, color=z)) + 
    geom_bar(data=sBitsFrame,alpha = 0.2,stat="identity", position="identity") + 
    labs(title="Shakespeare Bits Plain vs Encrypted", x="Bits", y="Frequency")
```

```{r}
sBytes = read.csv("shakespeare_bytes.csv")
ssBytes = read.csv("shakespeare_secret_bytes.csv")

bytesFrame <- data.frame(xx = c(sBytes$bytes, ssBytes$bytes), z = c(rep("Plain", nrow(sBytes)), rep("Encypted", nrow(ssBytes))))

ggplot(bytesFrame,aes(x=xx,color=z)) + 
    geom_histogram(data=bytesFrame,alpha = 0.2,position="identity", bins = 255) + 
    labs(title="Shakespeare Bytes Plain vs Encrypted", x="Bytes", y="Frequency")
```

Interestingly enough, we can see how encrypting the data turns both the bits and bytes into a uniform distribution. This would provide a baseline for encrypting any large set of values. 

## Conclusions

Assuming the machine we used to gather our data was functioning normally, we now have a rough baseline for the resting state of the Linux CADE machines. Packets/second are not modeled by a statistical distribution, but we can expect to see a mean of around `r packetMean`. Similarly, page faults are not measured by a statistical distribution, but we can expect a mean of around `r pageMean` These two tests are likely not modeled by a single statistical distributions as the probable number of packets/faults change depending on what tasks the computer is running in the background (which also explains the large standard deviations). Combined core temperatures are modeled by a normal distribution and have a combined mean of `r mean(means)`. Ping times are modeled by a gamma distribution and have a mean of `r pingMean`. Finally, encrypting a large set of data should produce a uniform distribution of both bits and bytes. If we run the test with specifically the Shakespeare data, it should produce the exact values shown in the data analysis section. 

If any machine in the lab shows significantly different stats, it would be worth investigating as it may be indicative of a potential problem. 



